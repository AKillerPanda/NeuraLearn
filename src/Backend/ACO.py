import logging
import time

import matplotlib.pyplot as plt
import numpy as np

from graph import KnowledgeGraph, TopicLevel

"""
Ant Colony Optimization for Learning Path Discovery  (Spectral + Vectorised)
-----------------------------------------------------------------------------
Given a KnowledgeGraph (DAG of topics with prerequisite edges), find the
optimal ordering of topics to study.  "Optimal" means:

  1. All prerequisite constraints are satisfied  (hard constraint).
  2. Difficulty transitions are smooth  (FOUNDATIONAL -> INTERMEDIATE -> ...).
  3. Closely related topics are studied together  (spectral locality bonus).
  4. The total "cognitive cost" of the path is minimised.

Performance — spectral graph theory + vectorised NumPy:
  • Spectral distances (Laplacian eigenvectors) replace naive graph-hop relatedness
  • Fiedler vector initialises pheromone so spectrally close topics start with
    higher pheromone (warm start converges 2-5x faster)
  • Spectral cluster bonus keeps ants within topic clusters
  • Cost / prereq / heuristic matrices built via broadcasting (no O(n^2) Python loops)
  • Prerequisite-availability check is a single boolean matmul per step
  • Path scoring and pheromone deposit use fancy indexing
  • Pre-allocated walk buffers are reused across ants to avoid GC pressure
"""

# ---------------------------------------------------------------------------
# Numeric difficulty for smooth-transition scoring
# ---------------------------------------------------------------------------
_LEVEL_COST: dict[TopicLevel, int] = {
    TopicLevel.FOUNDATIONAL: 0,
    TopicLevel.INTERMEDIATE: 1,
    TopicLevel.ADVANCED:     2,
    TopicLevel.EXPERT:       3,
}


# ---------------------------------------------------------------------------
# Vectorised matrix construction
# ---------------------------------------------------------------------------
def _build_matrices(
    kg: KnowledgeGraph,
) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, dict[int, int], np.ndarray, np.ndarray, np.ndarray]:
    """
    Build all dense K x K matrices used by the ACO in one pass.

    Uses spectral graph theory for intelligent relatedness scoring:
      - Spectral distances (from Laplacian eigenvectors) measure how
        topologically close two topics are, even when not directly connected.
      - Spectral cluster labels group topics for locality bonus.

    Returns
    -------
    cost          (K, K)  float64  cognitive transition cost
    prereq        (K, K)  bool     prereq[i, j] <=> j is a prerequisite of i
    eta           (K, K)  float64  heuristic desirability (1 / cost)
    start_weights (K,)    float64  initial-step weights (prefer foundational)
    id_to_idx     dict    topic_id -> dense index 0..K-1
    idx_to_id     (K,)    int64    dense index -> topic_id
    spectral_dist (K, K)  float64  pairwise spectral distances
    cluster_labels (K,)   int32    spectral cluster assignment
    """
    ids = sorted(kg.topics.keys())
    K = len(ids)
    id_to_idx: dict[int, int] = {tid: i for i, tid in enumerate(ids)}
    idx_to_id = np.array(ids, dtype=np.int64)

    if K == 0:
        empty_f = np.empty((0, 0), dtype=np.float64)
        return (empty_f, np.empty((0, 0), dtype=bool), empty_f, np.empty(0),
                id_to_idx, idx_to_id, empty_f, np.empty(0, dtype=np.int32))

    # ---- level vector (one Python pass, then all-vectorised) ---------------
    level_vec = np.array(
        [_LEVEL_COST[kg.topics[tid].level] for tid in ids], dtype=np.float64,
    )

    # ---- difficulty cost via broadcasting  O(K^2) in C ----------------------
    diff = level_vec[np.newaxis, :] - level_vec[:, np.newaxis]      # (K, K)
    difficulty_cost = np.where(diff < 0, np.abs(diff) * 3.0, diff)  # regression x3

    # ---- spectral relatedness (replaces naive -0.5 edge bonus) -------------
    # Spectral distances capture global graph topology, not just direct edges
    try:
        full_spec_dist = kg.spectral_distances()    # (N, N) where N >= K
        # extract only the rows/cols for our topic ids
        spec_dist = full_spec_dist[np.ix_(ids, ids)]  # (K, K)
        # normalise to [0, 1]
        sd_max = spec_dist.max()
        if sd_max > 1e-12:
            spec_dist_norm = spec_dist / sd_max
        else:
            spec_dist_norm = np.zeros_like(spec_dist)
        # relatedness: closer in spectral space -> larger negative bonus
        relatedness = -0.8 * (1.0 - spec_dist_norm)
    except Exception:
        # Fallback: direct-edge bonus (original behaviour)
        relatedness = np.zeros((K, K), dtype=np.float64)
        spec_dist = np.ones((K, K), dtype=np.float64)
        for i, tid in enumerate(ids):
            unlocks = kg.topics[tid].unlocks
            if unlocks:
                cols = [id_to_idx[u] for u in unlocks if u in id_to_idx]
                if cols:
                    relatedness[i, cols] = -0.5

    # ---- spectral clustering for group bonus --------------------------------
    try:
        n_clust = max(2, min(K // 3, 5))
        full_labels = kg.spectral_clustering(n_clusters=n_clust)
        cluster_labels = full_labels[ids].astype(np.int32)
    except Exception:
        cluster_labels = np.zeros(K, dtype=np.int32)

    # Add cluster bonus: same cluster -> small extra bonus
    same_cluster = (cluster_labels[:, None] == cluster_labels[None, :]).astype(np.float64)
    np.fill_diagonal(same_cluster, 0.0)
    relatedness -= 0.2 * same_cluster

    # ---- prerequisite boolean matrix (vectorised where possible) -----------
    prereq = np.zeros((K, K), dtype=bool)
    for i, tid in enumerate(ids):
        pset = kg.topics[tid].prerequisites
        if pset:
            cols = [id_to_idx[p] for p in pset if p in id_to_idx]
            if cols:
                prereq[i, cols] = True

    # ---- final cost & heuristic --------------------------------------------
    cost = 1.0 + difficulty_cost + relatedness
    np.clip(cost, 0.1, None, out=cost)  # ensure positive costs
    np.fill_diagonal(cost, 1e6)
    eta = 1.0 / (cost + 1e-10)

    # ---- start weights (foundational preferred) ----------------------------
    start_weights = 1.0 / (level_vec + 1.0)

    return cost, prereq, eta, start_weights, id_to_idx, idx_to_id, spec_dist, cluster_labels


# ---------------------------------------------------------------------------
# ACO for Learning Paths
# ---------------------------------------------------------------------------
class LearningPathACO:
    """
    Ant Colony Optimization over a KnowledgeGraph  (vectorised).

    Internally uses dense 0..K-1 indices for compact, cache-friendly matrices.
    External API returns topic_ids / topic names.

    Parameters
    ----------
    kg         : KnowledgeGraph to optimise over
    m          : number of ants per iteration
    k_max      : maximum iterations
    alpha      : pheromone importance (higher → follow the colony)
    beta       : heuristic importance (higher → follow cost matrix)
    rho        : evaporation rate  (0 = none, 1 = full)
    Q          : pheromone deposit constant
    time_limit : wall-clock seconds before early stop
    """

    __slots__ = (
        "kg", "K",
        "cost", "prereq", "eta", "_start_weights",
        "_id_to_idx", "_idx_to_id",
        "_spectral_dist", "_cluster_labels",
        "m", "k_max", "alpha", "beta", "rho", "Q", "time_limit",
        "tau", "A",
        "history", "best_path", "best_cost",
        "_rng",
    )

    def __init__(self, kg: KnowledgeGraph, **kwargs) -> None:
        self.kg = kg

        # Vectorised matrix construction with spectral heuristics
        (
            self.cost, self.prereq, self.eta, self._start_weights,
            self._id_to_idx, self._idx_to_id,
            self._spectral_dist, self._cluster_labels,
        ) = _build_matrices(kg)
        self.K: int = len(self._idx_to_id)

        # Hyper-parameters
        self.m: int          = kwargs.get("m", 50)
        self.k_max: int      = kwargs.get("k_max", 80)
        self.alpha: float    = kwargs.get("alpha", 1.0)
        self.beta: float     = kwargs.get("beta", 3.0)
        self.rho: float      = kwargs.get("rho", 0.85)
        self.Q: float        = kwargs.get("Q", 10.0)
        self.time_limit: int = kwargs.get("time_limit", 10)

        # Pheromone matrix — Fiedler-based warm start
        # Topics close in Fiedler-vector space get higher initial pheromone
        # This biases early exploration towards spectrally coherent paths
        self.tau: np.ndarray = np.ones((self.K, self.K), dtype=np.float64)
        if self.K > 2:
            try:
                fiedler = kg.fiedler_vector()
                fids = np.array([kg.topics[tid].topic_id for tid in sorted(kg.topics.keys())],
                                dtype=np.int64)
                fv = fiedler[fids]  # extract values for our topics
                # Fiedler distance: |f_i - f_j| — small = same partition
                fd = np.abs(fv[:, None] - fv[None, :])
                fd_max = fd.max()
                if fd_max > 1e-12:
                    fd_norm = fd / fd_max
                else:
                    fd_norm = np.zeros_like(fd)
                # Warm start: tau_ij = 1.0 + 1.5 * (1 - normalised_fiedler_dist)
                self.tau = 1.0 + 1.5 * (1.0 - fd_norm)
            except Exception:
                pass  # fall back to uniform pheromone

        self.A: np.ndarray = np.empty_like(self.tau)

        # Results
        self.history: list[float] = []
        self.best_path: np.ndarray = np.empty(0, dtype=np.int64)
        self.best_cost: float = float("inf")

        # RNG (modern numpy generator)
        self._rng = np.random.default_rng()

    # ---- attractiveness (fully vectorised, in-place) ----------------------
    def _compute_attractiveness(self) -> None:
        """A = τ^α · η^β  (element-wise, in-place where possible)."""
        np.power(self.tau, self.alpha, out=self.A)
        self.A *= np.power(self.eta, self.beta)

    # ---- prerequisite-aware candidates (vectorised boolean matmul) --------
    def _get_available(self, visited: np.ndarray) -> np.ndarray:
        """
        Return dense indices of topics whose prereqs are all visited.

        Parameters
        ----------
        visited : (K,) bool — True for topics already in the path.

        Returns
        -------
        (M,) int64 — dense indices of available (unlocked & unvisited) topics.
        """
        # unmet_prereqs[i] = count of prereqs of i that are NOT visited
        unmet = (self.prereq & ~visited).sum(axis=1)     # (K,)
        return np.flatnonzero((unmet == 0) & ~visited)

    # ---- single ant walk (buffer-reusing) ---------------------------------
    def _ant_walk(
        self,
        visited_buf: np.ndarray,
        path_buf: np.ndarray,
    ) -> tuple[int, float]:
        """
        Build one complete learning path.

        Writes into pre-allocated *path_buf* and returns
        (path_length, total_cost).  Buffers are zeroed at entry.
        """
        visited_buf[:] = False
        path_len = 0
        rng = self._rng

        while path_len < self.K:
            candidates = self._get_available(visited_buf)
            if candidates.size == 0:
                break  # stuck — should not happen on a valid DAG

            if path_len == 0:
                # First step: prefer foundational topics
                weights = self._start_weights[candidates]
            else:
                # Vectorised row-slice of attractiveness matrix
                weights = self.A[path_buf[path_len - 1], candidates]

            # Ensure strictly positive weights
            w_min = weights.min()
            if w_min <= 0.0:
                weights = weights - w_min + 1e-10

            # Weighted random selection via cumsum + searchsorted (no Python loop)
            cumsum = weights.cumsum()
            r = rng.random() * cumsum[-1]
            chosen = candidates[np.searchsorted(cumsum, r)]

            path_buf[path_len] = chosen
            visited_buf[chosen] = True
            path_len += 1

        # ---- vectorised scoring -------------------------------------------
        if path_len < 2:
            return path_len, 0.0
        p = path_buf[:path_len]
        total_cost = float(self.cost[p[:-1], p[1:]].sum())  # fancy indexing
        return path_len, total_cost

    # ---- main loop --------------------------------------------------------
    def optimise(self) -> tuple[list[int], float]:
        """
        Run the ACO with aggressive early stopping.

        Convergence heuristics (millisecond-speed for typical graphs):
          • Stagnation detector: stop if the best cost hasn't improved for
            `patience` consecutive iterations.
          • Wall-clock time limit (self.time_limit seconds).

        Returns
        -------
        best_path : list[int]  topic_ids in optimal learning order
        best_cost : float      total cognitive transition cost
        """
        if self.K == 0:
            return [], 0.0
        if self.K == 1:
            # Trivial: only one topic
            return [int(self._idx_to_id[0])], 0.0

        start_time = time.time()
        patience = max(5, self.k_max // 5)   # stop after N stagnant iterations
        stagnant = 0

        # Pre-allocate walk buffers (reused every ant walk — zero GC)
        visited_buf = np.zeros(self.K, dtype=bool)
        path_buf    = np.empty(self.K, dtype=np.int64)

        prev_best = float("inf")

        for iteration in range(self.k_max):
            self._compute_attractiveness()

            # Evaporate pheromone (vectorised in-place)
            self.tau *= (1.0 - self.rho)
            np.clip(self.tau, 0.01, None, out=self.tau)

            for _ in range(self.m):
                path_len, cost = self._ant_walk(visited_buf, path_buf)

                # Vectorised pheromone deposit via fancy indexing
                if path_len >= 2:
                    deposit = self.Q / max(cost, 1e-10)
                    p = path_buf[:path_len]
                    self.tau[p[:-1], p[1:]] += deposit

                # Track best
                if cost < self.best_cost and path_len == self.K:
                    self.best_cost = cost
                    self.best_path = path_buf[:path_len].copy()

            self.history.append(self.best_cost)

            # ── Stagnation check ──────────────────────────────────────
            if abs(prev_best - self.best_cost) < 1e-6:
                stagnant += 1
            else:
                stagnant = 0
            prev_best = self.best_cost

            if stagnant >= patience:
                logging.debug("ACO converged (stagnant %d iters) at iter %d", patience, iteration + 1)
                break

            if time.time() - start_time > self.time_limit:
                logging.debug("ACO time limit after %d iterations.", iteration + 1)
                break

        # Map dense indices → topic_ids
        result_ids = self._idx_to_id[self.best_path].tolist() if self.best_path.size else []
        return result_ids, self.best_cost

    # ---- results ----------------------------------------------------------
    def get_named_path(self) -> list[str]:
        """Return the best path as a list of topic names."""
        if self.best_path.size == 0:
            return []
        ids = self._idx_to_id[self.best_path]
        return [self.kg.topics[int(tid)].name for tid in ids]

    def print_result(self) -> None:
        """Pretty-print the optimal learning path."""
        print(f"\n{'='*60}")
        print(f"  ACO Optimal Learning Path  (cost: {self.best_cost:.2f})")
        print(f"{'='*60}")
        if self.best_path.size == 0:
            print("  (no path found)")
        else:
            ids = self._idx_to_id[self.best_path]
            for step, tid in enumerate(ids, 1):
                t = self.kg.topics[int(tid)]
                prereqs = (
                    ", ".join(self.kg.topics[p].name for p in t.prerequisites)
                    or "none"
                )
                print(f"  {step:>2}. {t.name:<40} [{t.level.name}]  prereqs: {prereqs}")
        print(f"{'='*60}\n")

    def plot_convergence(self, save_path: str | None = None) -> None:
        """Plot cost over iterations to visualise ACO convergence."""
        if not self.history:
            print("No history to plot — run optimise() first.")
            return
        fig, ax = plt.subplots(figsize=(8, 4))
        ax.plot(
            range(1, len(self.history) + 1), self.history,
            marker="o", markersize=3,
        )
        ax.set_xlabel("Iteration")
        ax.set_ylabel("Best Path Cost")
        ax.set_title("ACO Convergence — Learning Path Optimisation")
        ax.grid(True, alpha=0.3)
        plt.tight_layout()
        if save_path:
            fig.savefig(save_path, dpi=150)
            print(f"Saved convergence plot to {save_path}")
        plt.show()


# ---------------------------------------------------------------------------
# Convenience: scrape + graph + ACO in one call
# ---------------------------------------------------------------------------
def find_optimal_learning_path(
    topic: str,
    **aco_kwargs,
) -> tuple[list[str], float, LearningPathACO]:
    """
    End-to-end: scrape a topic → build knowledge graph → run ACO → return
    the optimal learning path as a list of topic names.

    Returns (named_path, cost, aco_instance).
    """
    from Webscraping import get_learning_spec

    spec = get_learning_spec(topic)
    kg = KnowledgeGraph.from_spec(spec)
    aco = LearningPathACO(kg, **aco_kwargs)
    aco.optimise()
    return aco.get_named_path(), aco.best_cost, aco


# ---------------------------------------------------------------------------
# Demo
# ---------------------------------------------------------------------------
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(message)s")

    # --- Build a sample knowledge graph ------------------------------------
    from graph import TopicLevel as TL

    kg = KnowledgeGraph()
    overview     = kg.create_topic("Overview",                  level=TL.FOUNDATIONAL)
    history      = kg.create_topic("History",                   level=TL.FOUNDATIONAL)
    communication = kg.create_topic("In communication",         level=TL.FOUNDATIONAL)
    manuscripts  = kg.create_topic("In manuscripts",            level=TL.FOUNDATIONAL)
    science      = kg.create_topic("In science",                level=TL.INTERMEDIATE)
    expression   = kg.create_topic("As artistic expression",    level=TL.INTERMEDIATE)
    artists      = kg.create_topic("Notable artists",           level=TL.INTERMEDIATE)
    materials    = kg.create_topic("Materials",                  level=TL.INTERMEDIATE)
    technique    = kg.create_topic("Technique",                  level=TL.ADVANCED)
    tone         = kg.create_topic("Tone",                       level=TL.ADVANCED)
    form         = kg.create_topic("Form and proportion",        level=TL.ADVANCED)
    perspective  = kg.create_topic("Perspective",                level=TL.EXPERT)
    composition  = kg.create_topic("Composition",                level=TL.EXPERT)
    process      = kg.create_topic("Process",                    level=TL.EXPERT)

    # prerequisite edges (linear chain)
    all_topics = [
        overview, history, communication, manuscripts, science, expression,
        artists, materials, technique, tone, form, perspective,
        composition, process,
    ]
    edges = [(all_topics[i].topic_id, all_topics[i+1].topic_id) for i in range(len(all_topics)-1)]
    kg.add_prerequisites_bulk(edges)

    print("--- Knowledge Graph ---")
    kg.print_curriculum()

    print("\n--- Running ACO ---")
    aco = LearningPathACO(kg, m=50, k_max=60, time_limit=8)
    best_path, best_cost = aco.optimise()

    aco.print_result()

    print(f"Convergence: started at {aco.history[0]:.2f}, ended at {aco.history[-1]:.2f}")
    print(f"Improvement: {((aco.history[0] - aco.history[-1]) / aco.history[0] * 100):.1f}%")

    # --- Also demo the end-to-end convenience function ---------------------
    print("\n\n--- End-to-end: find_optimal_learning_path('Piano') ---")
    named_path, cost, _ = find_optimal_learning_path("Piano", m=30, k_max=40)
    print(f"Cost: {cost:.2f}")
    for i, name in enumerate(named_path, 1):
        print(f"  {i}. {name}")
